DEBUG:RL-CW Main:Config: {'overall': {'agent_name': 'ddpg', 'env_name': 'LunarLander-v2', 'context_capacity': 64, 'output': {'verbose': True, 'render': False, 'save': {'episodes': 5, 'csv': True, 'charts': True, 'raw': False, 'recordings': True}}, 'episodes': {'max': 2}, 'timesteps': {'max': 1000000000000000, 'start_training': 50}}, 'agents': {'random': {'foo': 'bar'}, 'ddpg': {'alpha': 2.5e-05, 'beta': 0.00025, 'gamma': 0.99, 'tau': 0.001, 'batch_size': 50, 'input_dims': [8], 'layer1_size': 400, 'layer2_size': 300, 'n_actions': 2, 'max_size': 1000000}, 'sac': {'sample_size': 50, 'batch_size': 64, 'learning_rate': 0.0001, 'alpha': 0.9, 'gamma': 0.99, 'scale': 2, 'tau': 0.05, 'nn_initial_weights': 0.0003, 'actor_noise': 1e-06, 'no_hidden_neurons': 256}}}
INFO:RL-CW Main:CUDA is enabled!
INFO:RL-CW Main:Marking episodes [0, 0, 0, 1, 1, 1] for saving...
INFO:RL-CW Orchestrator:Running agent ddpg ...
INFO:RL-CW Runner:Episode Summary for 0 (Cumulative, Avg, No Timesteps): (-1184.041390212338, -12.20661227023029, 97)
INFO:RL-CW Runner:Episode Summary for 1 (Cumulative, Avg, No Timesteps): (-756.5799777465394, -12.823389453331176, 59)
INFO:RL-CW Evaluator:Cumulative Rewards: [-1184.041390212338, -756.5799777465394]
INFO:RL-CW Evaluator:Average Rewards: [-12.20661227023029, -12.823389453331176]
INFO:RL-CW Evaluator:No Timesteps: [97, 59]
